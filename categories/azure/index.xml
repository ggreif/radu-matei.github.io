<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Radu Matei - Developer Evangelist</title>
    <link>http://radu-matei.github.io/categories/azure/index.xml</link>
    <description>Recent content on Radu Matei - Developer Evangelist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://radu-matei.github.io/categories/azure/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Hybrid Cluster: A CI/CD Story [Part 1] - Configuring a hybrid swarm mode cluster in Azure with acs-engine</title>
      <link>http://radu-matei.github.io/blog/hybrid-swarmmode/</link>
      <pubDate>Mon, 03 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://radu-matei.github.io/blog/hybrid-swarmmode/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is the first part in our (at least) two parts describing how to get started with a hybrid Docker Swarm Mode cluster. In this first part, we will focus on deploying a hybrid cluster on Azure.&lt;/p&gt;

&lt;p&gt;Now, you can create yourself a hybrid cluster within any private network where you have a Windows Server 2016 with Containers and a Linux machine - it can be locally, with VirtualBox, Hyper-V or VMWare, or it can be on your cloud provider of choice. The simplicity of Docker Swarm allows us to easily create a swarm within minutes of having our VMs deployed.&lt;/p&gt;

&lt;p&gt;Here is a list of resources you might want to get started with before diving into this article:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/swarm/swarm-tutorial/&#34;&gt;Getting started with Swarm Mode and Linux Containers - Docker docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/swarm-mode&#34;&gt;Getting started with Swarm Mode and Windows Containers - Microsoft docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/swarm-mode#linuxwindows-mixed-os-clusters&#34;&gt;Initializing a Linux+Windows mixed-os cluster - Microsoft docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-is-this-article-different-compared-to-the-docs-above&#34;&gt;How is this article different compared to the docs above?&lt;/h2&gt;

&lt;p&gt;In this article we will focus on deploying the cluster on Azure programatically, using &lt;a href=&#34;https://github.com/Azure/acs-engine&#34;&gt;acs-engine&lt;/a&gt;, a tool that generates &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview&#34;&gt;ARM (Azure Resource Manager) templates&lt;/a&gt; for Docker enabled clusters on Microsoft Azure. It will also deploy all resources necessary for our cluster, like load balancers, configure DNS for masters and agents and scale sets for agents and masters. More on this later.&lt;/p&gt;

&lt;p&gt;While you can &lt;a href=&#34;https://github.com/Azure/acs-engine&#34;&gt;find more information about acs-engine on the GitHub repo&lt;/a&gt;, in short, the tool takes a cluster definition file and outputs ARM templates that can be deployed using the &lt;a href=&#34;https://azure.github.io/projects/clis/&#34;&gt;various Azure command-line interfaces&lt;/a&gt; like Azure CLI 2.0 or Azure PowerShell.&lt;/p&gt;

&lt;h2 id=&#34;getting-started-prerequisites&#34;&gt;Getting started - prerequisites&lt;/h2&gt;

&lt;p&gt;This article will continue under the assumption that you have an active Azure subscription. If you don&amp;rsquo;t, there are various ways to get a free subscription, like &lt;a href=&#34;https://www.visualstudio.com/dev-essentials/&#34;&gt;Visual Studio Dev Essentials&lt;/a&gt; (see &lt;a href=&#34;https://github.com/awesome-opening-opportunities/technical-documentation/blob/master/docs/vs-dev-essentials.md&#34;&gt;this link on how to activate your free monthly $25&lt;/a&gt;), or a &lt;a href=&#34;https://azure.microsoft.com/en-us/free/&#34;&gt;free trial&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Before you get started, there is a &lt;a href=&#34;https://channel9.msdn.com/Events/DXPortugal/OSCAMP-Open-Source-Software-powered-by-Bright-Pixel/The-Hybrid-Swarm-Running-Windows-and-Linux-Apps-in-one-Docker-Cluster&#34;&gt;great talk by Docker Developer Advocate and Microsoft MVP Elton Stoneman titled: The Hybrid Swarm: Running Windows and Linux Apps in one Docker Cluster&lt;/a&gt; where he talks about the concepts involved in having a hybrid swarm cluster and that I highly recommend.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;understanding-all-types-of-containers&#34;&gt;Understanding all types of containers&lt;/h2&gt;

&lt;p&gt;First, there are Linux containers. They have been around for a while now (no, Docker did not invent them) and Docker created awesome tooling and integrations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/journey.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/allthingscontainer/2016/10/14/why-containers/&#34;&gt;Photo credits to Bruno Terkaly, from this article&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Linux containers use the host kernel to run &amp;ldquo;containerized&amp;rdquo; workloads - that is execute the process inside the container using Linux kernel features like cgroups and namespaces. Of course, to run Linux containers you need a Linux kernel - this hasn&amp;rsquo;t changed and will not change any time soon.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/&#34;&gt;When we talk about the Windows ecosystem&lt;/a&gt;, we have Windows Server Containers and Hyper-V Containers.&lt;/p&gt;

&lt;p&gt;Windows Server Containers, much like Linux containers, share the kernel with the host and other containers. &amp;ldquo;These containers do not provide a hostile security boundary and should not be used to isolate untrusted code.&amp;rdquo; (&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/&#34;&gt;source - Microsoft docs&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Hyper-V Containers - &amp;ldquo;expands on the isolation provided by Windows Server Containers by running each container in a highly optimized virtual machine. In this configuration, the kernel of the container host is not shared with other containers on the same host. These containers are designed for hostile multitenant hosting with the same security assurances of a virtual machine. Since these containers do not share the kernel with the host or other containers on the host, they can run kernels with different versions and configurations.&amp;rdquo; (&lt;a href=&#34;https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/&#34;&gt;source - Microsoft docs&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;However, there&amp;rsquo;s a twist: announced at DockerCon 2017, you will be able to run Linux containers on Windows hosts using Hyper-V Isolation&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/win-linux-containers.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://thenewstack.io/finally-linux-containers-really-will-run-windows-linuxkit/&#34;&gt;Image from The New Stack&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is all possible through the new &lt;a href=&#34;https://github.com/linuxkit/linuxkit&#34;&gt;LinuxKit project&lt;/a&gt;, but more on this on a separate article in the future.&lt;/p&gt;

&lt;p&gt;After we deploy our cluster, we will be able to deploy all types of containers described above.&lt;/p&gt;

&lt;h2 id=&#34;the-acs-engine-cluster-definition&#34;&gt;The acs-engine cluster definition&lt;/h2&gt;

&lt;p&gt;As said earlier, we will use a JSON cluster definition file to, well, define our cluster.&lt;/p&gt;

&lt;script src=&#34;//gist.github.com/radu-matei/f610287201e4c08eb2e69eb5ebd02b2f.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is a pretty standard cluster definition file for acs-engine, except for the addition of &lt;code&gt;windowspool&lt;/code&gt;, a pool of Windows Server agents in our cluster.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can find &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/master/docs/clusterdefinition.md&#34;&gt;in-depth documentation for the cluster definition on the acs-engine GitHub repo here.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From the definition file, we see that we have a Swarm Mode cluster, with 3 Linux masters, 3 Linux agents and 3 Windows Server 2016 agents. Before we can use this definition file, we need to add the required values for &lt;code&gt;dnxPrefix&lt;/code&gt; for the masters, Linux and Windows agents.&lt;/p&gt;

&lt;p&gt;You must also provide a username and public SSH key for the Linux VMs and a username and password for the Windows VMs, and you can change the default number of 3 for the agent and master count.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/acs-swarmmode.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to the image above, there is an additional VM Scale Set with the Windows agents. All VMs are in the same VNET, with the masters on a private subnet. All VMs are fully accessible to each other.&lt;/p&gt;

&lt;h2 id=&#34;deploying-the-cluster-to-azure&#34;&gt;Deploying the cluster to Azure&lt;/h2&gt;

&lt;p&gt;So far we only have a cluster definition (with values for FQDN, usernames and passwords). Before we can actually deploy, we need to generate the ARM templates using acs-engine.&lt;/p&gt;

&lt;p&gt;In order to do this, we will use the &lt;code&gt;acs-engine&lt;/code&gt; tool. After we have the ARM template, we will use the &lt;code&gt;az&lt;/code&gt; CLI to deploy them. You could install these either locally, or within containers, but the easiest way to do it is to use the &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/cloud-shell/overview&#34;&gt;Azure Cloud Shell&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/cloud-shell.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;List of tools and languages supported in the Azure Cloud Shell&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Log into &lt;a href=&#34;https://portal.azure.com&#34;&gt;portal.azure.com&lt;/a&gt; and request a cloud shell. You should see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/portal-shell.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we should &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/master/docs/acsengine.md#downloading-and-building-acs-engine&#34;&gt;follow the instructions in the acs-engine documentation&lt;/a&gt; and install acs-engine in the Azure Cloud Shell.&lt;/p&gt;

&lt;p&gt;First, we need to create a new directory called &lt;code&gt;go&lt;/code&gt; and set it as &lt;code&gt;GOPATH&lt;/code&gt;: &lt;code&gt;mkdir go&lt;/code&gt; and &lt;code&gt;export GOPATH=/home/{yourusername}/go&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then, we need to download the package for acs-engine: &lt;code&gt;go get github.com/Azure/acs-engine&lt;/code&gt;, then navigate to the source of the package and build it: &lt;code&gt;go build&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then, we add the &lt;code&gt;bin&lt;/code&gt; folder from the &lt;code&gt;go&lt;/code&gt; directory in the path: &lt;code&gt;export PATH=$PATH:/home/{yourusername}/go/bin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now you should be able to execute &lt;code&gt;acs-engine&lt;/code&gt; from any directory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/acs-engine.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s create the ARM templates we will deploy: in a new directory, download the gist with the initial cluster definition (the gist file from above). You can either copy it yourself, or &lt;code&gt;wget&lt;/code&gt; the file:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;wget https://gist.githubusercontent.com/radu-matei/f610287201e4c08eb2e69eb5ebd02b2f/raw/d6a30f867b09d4baa64f78d2499a154096d053e2/swarmmode-hybrid.json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After you edit the file with your values, generate the ARM templates using &lt;code&gt;acs-engine generate swarmmode-hybrid.json&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/acs-engine-generate.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This will create an &lt;code&gt;_output&lt;/code&gt; directory that will contain the ARM template tht we will use for the deployment.&lt;/p&gt;

&lt;p&gt;First of all, we will create a new resource group: &lt;code&gt;az group create --location westeurope --name your-resourcegroup-name&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you should choose the region closest to your location.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then, using the generated files &lt;code&gt;azuredeploy.json&lt;/code&gt; and &lt;code&gt;azuredeploy.parameters.json&lt;/code&gt;, create a new deployment using the &lt;code&gt;az&lt;/code&gt; command-line interface:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az group deployment create --name hybrid-swarmmode-deployment --resource-group {your-resource-group} --template-file azuredeploy.json  --parameters azuredeploy.parameters.json&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you can also use a local installation of &lt;code&gt;az&lt;/code&gt;, or in a container, or any method of deploying ARM templates.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After the deployment started, here is how the resource group should look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/rg.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice the resources created in the resource group:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 public IPs for masters, Linux agents and Windows agents&lt;/li&gt;
&lt;li&gt;load balancers for masters, Linux agents and Windows agents&lt;/li&gt;
&lt;li&gt;VM scale sets for the agents and availability sets&lt;/li&gt;
&lt;li&gt;network interfaces and OS disks for the masters&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;connecting-to-the-cluster&#34;&gt;Connecting to the cluster&lt;/h2&gt;

&lt;p&gt;After the deployment succeeds, you are now ready to connect to the master. You will SSH into the masters using the user and SSH key you setup in the cluster definition file. The 3 FQDNs will have the following template:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{yourfqdnname}.{azurelocation}.cloudapp.azure.com&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Each master can be publicly accessed using the FQDN and one of the ports (2200..220x) (So you will access the first master on 2200, the second master on 2201 and so on.). For example, to SSH into the first master, use the following:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssh -i path-to-private-key azureuser@{yourfqdn}.{azurelocation}.cloudapp.azure.com -p 2200&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then, if you list all nodes in the cluster you might first see this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/docker-node-ls.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This means we only see the 3 masters and the 3 Linux agents. This means that even though the Windows nodes were deployed, they did not join the swarm.&lt;/p&gt;

&lt;p&gt;A very quick solution is to reimage the Windows agents. This means restoring them to the initial state and executing all scripts that were executed when initializing the cluster.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A very probable cause of this could be that &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/dd2edf94e182dd9006ddf3fa8f8388b4e5a1eed5/parts/Install-ContainerHost-And-Join-Swarm.ps1&#34;&gt;the script that joins the Windows agents to the cluster&lt;/a&gt; might get executed before the masters actually start.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After reimaging or restarting the VMs, your cluster should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/node-ls-wc.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now you have a full hybrid Swarm Mode cluster, with some Windows agents, as well as Linux ones:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/node-inspect.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;deploying-services-to-the-cluster&#34;&gt;Deploying services to the cluster&lt;/h2&gt;

&lt;p&gt;From now on, you can treat this cluster as any other Docker Swarm Mode cluster: with the single mention that you cannot run Linux containers on Windows and Windows containers on Linux. This means that when starting services, we need to put some restrictions in place.&lt;/p&gt;

&lt;p&gt;We will deploy a simple Python web application on Linux that will use a Redis data store that we will run on Windows.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft-dx/docker-lab/tree/master/apps/python-redis&#34;&gt;The Python application can be found here&lt;/a&gt; and is very similar to the &lt;a href=&#34;https://docs.docker.com/compose/gettingstarted/&#34;&gt;Docker Compose one from the Official Docker Docs&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script src=&#34;//gist.github.com/radu-matei/7543e906e3633075cd32231e46628bf1.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The most important thing to notice in the stack file is the deployment constraint on the node operating system. As stated earlier, this is very important in the stack file as a Linux service will not run in a Windows host and vice-versa.&lt;/p&gt;

&lt;p&gt;You can see that the &lt;code&gt;redis&lt;/code&gt; service is based on the Windows version of Redis (not something that you would use in production, here just for showcase) and is based on the Nano Server image.&lt;/p&gt;

&lt;p&gt;To deploy this on the master, you need the file above. You can copy it, or &lt;code&gt;wget&lt;/code&gt; it directly: &lt;code&gt;wget https://gist.githubusercontent.com/radu-matei/7543e906e3633075cd32231e46628bf1/raw/f5e06e372c9a5c57f555e8580eee1c1a5ccb635e/hybrid-stack.yml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then, you need to &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/stack_deploy/&#34;&gt;create a new stack deployment&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker stack deploy --compose-file hybrid-stack.yml python-redis&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/stack-deploy.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This will create two new services, the web and Redis ones, and a new network for them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/service-ls.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/library/redis/&#34;&gt;Since the Nanoserver Redis image&lt;/a&gt; is around 340 MB, it will take a little to pull it, then start a container.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now since the application that exposes ports is the one running on a Linux node (the web application), we will access it on the port 80 (the one exposed) of the Linux agent FQDN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/hybrid-swarmmode/running.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article we saw how to deploy a hybrid Swarm Mode cluster on Azure using acs-engine and how to deploy a mixed-OS containerized application on the cluster we created.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Next, we will explore how to create a consistent CI/CD story with GitHub and Jenkins (with Linux and Windows slaves that are created dynamically for each build).&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;If you think this article could be better, please provide your feedback in the comments below.&lt;/p&gt;

&lt;h2 id=&#34;thanks-for-reading&#34;&gt;Thanks for reading :)&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Creating a CI/CD workflow with Kubernetes, Jenkins and Azure Container Service</title>
      <link>http://radu-matei.github.io/blog/kubernetes-jenkins-azure/</link>
      <pubDate>Tue, 11 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://radu-matei.github.io/blog/kubernetes-jenkins-azure/</guid>
      <description>

&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deploying-a-kubernetes-cluster-on-azure-container-service&#34;&gt;Deploying a Kubernetes cluster on Azure Container Service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-and-configuring-the-kubernetes-cli&#34;&gt;Installing and configuring the Kubernetes CLI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deploying-a-jenkins-master-on-the-cluster&#34;&gt;Deploying a Jenkins master on the cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configuring-jenkins-to-work-with-kubernetes&#34;&gt;Configuring Jenkins to work with Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configuring-jenkins-to-dinamically-spawn-agents-docker-containers-for-builds&#34;&gt;Configuring Jenkins to dinamically spawn agents (Docker containers) for builds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-happening-behind-the-scenes&#34;&gt;What is happening behind the scenes?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-docker-image-for-the-slaves&#34;&gt;The Docker image for the slaves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#next-steps&#34;&gt;Next Steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feedback&#34;&gt;Feedback&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/kubernetes-jenkins-azure/#deploying-a-jenkins-master-on-the-cluster&#34;&gt;If you already know how to deploy a Kubernetes cluster, please jump ahead to creating the Jenkins service.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The purpose of writing this article is to show how to deploy and configure a Kubernetes cluster on Azure Container Service and install the Jenkins master as a Kubernetes service that will spawn slaves to build your workloads.&lt;/p&gt;

&lt;p&gt;You can, of course, install Jenkins in a VM, but you loose all the flexibility that running Kubernetes gives you.&lt;/p&gt;

&lt;p&gt;Only the master will run continously to receive webhooks and to spawn (not sure if this is the right word :D) slaves (these will be also Docker containers) to build and deploy your updates.&lt;/p&gt;

&lt;h2 id=&#34;deploying-a-kubernetes-cluster-on-azure-container-service&#34;&gt;Deploying a Kubernetes cluster on Azure Container Service&lt;/h2&gt;

&lt;p&gt;The easiest way (in my opinion) to deploy a Kubernetes cluster is through &lt;a href=&#34;https://docs.microsoft.com/en-us/cli/azure/install-azure-cli&#34;&gt;the new Azure CLI 2.0&lt;/a&gt;.
Since you are reading an article about Kubernetes, I will go ahead and assume you are familiar with Docker, so I will use the Docker option to use the Azure CLI:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run -it -p 81:8080 azuresdk/azure-cli-python bash&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The reason I also mapped port 81 on the host (you can choose any available port on your machine) to port 8080 on the container (again, your choice) is because later we will create a proxy that will allow us to see the Kubernetes Dashboard. More on this later.&lt;/p&gt;

&lt;p&gt;You can find &lt;a href=&#34;https://github.com/Azure/azure-cli/blob/master/Dockerfile&#34;&gt;the Dockerfile for this image here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now you have a container with the new &lt;code&gt;az&lt;/code&gt; command line. First thing to do  - login to your Azure account using&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az login&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;After following the instructions in the command line (open a browser, go to &lt;a href=&#34;http://aka.ms/devicelogin&#34;&gt;http://aka.ms/devicelogin&lt;/a&gt; and paste the code from the console), you are ready to explore your Azure resources from the command line.&lt;/p&gt;

&lt;p&gt;In order to verify that the desired subscription is the default one (in case you have multpile subscriptions), you can execute&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az account show&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If it is not, you can change it by using&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az account set --subscription {subscription-id}&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now if you execute &lt;code&gt;az account show&lt;/code&gt;, you will see the one you selected.&lt;/p&gt;

&lt;p&gt;The first thing you shoud do before actually deploying the Kubernetes cluster is create a &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview#resource-groups&#34;&gt;resource group&lt;/a&gt;. This is done by executing the following command:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az group create --name kubernetes-jenkins --location westeurope&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you can select &lt;a href=&#34;https://azure.microsoft.com/en-us/regions/&#34;&gt;the closest Azure region&lt;/a&gt; to you when passing the parameter to &lt;code&gt;--location&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is how it should look like if the deployment succeeded:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/az-group-show.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you are ready to actually deploy the Kubernetes cluster:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az acs create --orchestrator-type=kubernetes --resource-group=kubernetes-jenkins-ci --name=kubernetes-jenkins --dns-prefix=kubernetesci --generate-ssh-keys&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that these instructions are also available on &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/container-service/container-service-kubernetes-walkthrough#create-your-kubernetes-cluster&#34;&gt;the official documentation for Kubernetes on Azure Container Service&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This will create a new Azure Container Service deployment in the resource group &lt;code&gt;kubernetes-jenkins&lt;/code&gt; with the name &lt;code&gt;kubernetes-jenkins&lt;/code&gt;, KLubernets as orchestrator and the DNS prefix of the master and nodes &lt;code&gt;kubernetesjenkins&lt;/code&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that this command will take around 10 minutes to complete.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point, you can either see the deployed resources from the command line by executing&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az acs show --resource-group kubernetes-jenkins --name kubernetes-jenkins&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Or by going to the &lt;a href=&#34;https://portal.azure.com&#34;&gt;Azure Portal&lt;/a&gt; to the Container Services blade:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/acs-cluster-portal.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can also see everything that got deployed for you (virtual machines, availability sets, storage accounts, network interfaces, network security groups, virtual networks, load balancers and route tables) by inspecting the resource group in the portal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/resource-group.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;installing-and-configuring-the-kubernetes-cli&#34;&gt;Installing and configuring the Kubernetes CLI&lt;/h2&gt;

&lt;p&gt;Next, you install the Kubernetes CLI, &lt;a href=&#34;https://kubernetes.io/docs/user-guide/kubectl-overview/&#34;&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt; by executing&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az acs kubernetes install-cli&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Azure/azure-cli/blob/master/src/command_modules/azure-cli-acs/azure/cli/command_modules/acs/custom.py#L273&#34;&gt;Since the Azure CLI is developed openly on GitHub, you can see exactly how the Azure CLI downloads and installs &lt;code&gt;kubectl&lt;/code&gt; here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then, you get the credentials for the cluster:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;az acs kubernetes get-credentials --resource-group=kubernetes-jenkins --name=kubernetes-jenkins&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;By deploying Kubernetes through Azure Container Service, the &lt;code&gt;az&lt;/code&gt; utilitary can manage the cluster credentials for you, but you can also use the configuration file you can find in &lt;code&gt;~/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, you are ready to use &lt;code&gt;kubectl&lt;/code&gt; as usual: &lt;code&gt;kubectl get nodes&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/kubectl-get-nodes.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For a more detailed walkthrough on creating the cluster and creating your first public services, make sure to &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/container-service/container-service-kubernetes-walkthrough&#34;&gt;complete this tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;deploying-a-jenkins-master-on-the-cluster&#34;&gt;Deploying a Jenkins master on the cluster&lt;/h2&gt;

&lt;p&gt;First of all, you need a &lt;code&gt;jenkins-master.yml&lt;/code&gt; file that describes the Jenkins service with persistent storage, public and private endpoints and resource limits. In order to get this file, simply execute the following command that will download the file from my Gist account:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;wget https://gist.githubusercontent.com/radu-matei/ccec29e108d0e01f50c8c1ea45a1dc58/raw/c32078736352dee3dbcf75e05f86fe801a4defe4/jenkins-master.yaml&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This file is based on the &lt;a href=&#34;https://github.com/jenkinsci/kubernetes-plugin/blob/master/src/main/kubernetes/gke.yml&#34;&gt;Jenkins documentation on deploying to Google Container Engine from GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This deployment will create a new namespace for the services, &lt;code&gt;kubernetes-plugin&lt;/code&gt; and will create the Jenkins master service based on the &lt;a href=&#34;https://hub.docker.com/r/jenkinsci/jenkins/&#34;&gt;jenkins:latest&lt;/a&gt; Docker image, and will expose ports 8080 (for the web interface) and 50000 (for communicating with the slaves), among with a persistent storage.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Take care when using the &lt;code&gt;:latest&lt;/code&gt; tag for images!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;kubectl create -f jenkins-master.yaml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Besides the containers and services this &lt;code&gt;.yaml&lt;/code&gt; file creates, there is also some persistent storage. If we look at the &lt;code&gt;jenkins-master.yaml&lt;/code&gt; in the persistent storage part, we can see that at some point, the original &lt;code&gt;gke.yaml&lt;/code&gt; creates a &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;&lt;code&gt;gcePersistentDisk&lt;/code&gt;&lt;/a&gt;, which is Google Cloud Platform&amp;rsquo;s specific storage. What happens here is that Jenkins needs some persistent storage in order to store configuration for the master, so that if the pod serving the master fails, it can recreate it based on the persistant volume.&lt;/p&gt;

&lt;p&gt;If we look at the &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;persistent volume claim&lt;/a&gt;, we see that it created an Azure Storage Account and a &lt;code&gt;.vhd&lt;/code&gt; where it deployed the storage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/pvc.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we now go in the Azure Portal, to the resource group we created for the cluster, we can find this storage account:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/storage-account.png&#34; alt=&#34;&#34; /&gt;
And if we open the blobs blade we can actually see the volume storage there:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/storage.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;configuring-jenkins-to-work-with-kubernetes&#34;&gt;Configuring Jenkins to work with Kubernetes&lt;/h2&gt;

&lt;p&gt;At this point, if we go to the Kubernetes dashboard, in the Services part we should see Jenkins with a public IP (or external endpoints) configured:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/jenkins-endpoints.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In order to see the Kubernetes dashboard you need to execute &lt;code&gt;kubectl proxy  --port 8080 --address=&#39;0.0.0.0&#39;&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If we navigate to the HTTP endpoint of Jenkins (that is port 80), we should see the first-time installation view from Jenkins:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/jenkins-1st.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In order to login we need to get a password from the machine running the service - in this case a Docker container running inside a Kubernetes pod. To see the pods either go in the Kubernetes dashboard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/pods.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Or you can see it from the command line, but first we need to set the CLI context to the newly created namespace:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl config set-context $(kubectl config current-context) --namespace=kubernetes-plugin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we can see the running pods:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl get pods&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/jenkins-pod.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Coming back to our previous task: retrieving the password that Jenkins set up at &lt;code&gt;/var/jenkins_home/secrets/initialAdminPassword&lt;/code&gt;. We need to execute a command inside this pod to get the password:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl exec -it jenkins-cqn0z cat /var/jenkins_home/secrets/initialAdminPassword&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/kubectl-get-password.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you should replace &lt;code&gt;jenkins-cqn0z&lt;/code&gt; from the &lt;code&gt;kubectl exec&lt;/code&gt; with your own pod!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now paste that string in the Jenkins setup page and you should be good to go.&lt;/p&gt;

&lt;p&gt;At this point, you would simply click on Install Suggested Plugins and it might work. It might also not work - &lt;code&gt;WARNING: No valid crumb was included in request&lt;/code&gt;, as in the photo below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/crumb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Try a few more times until it works and we will get solve this by enabling proxy compatibility once we have access to the service&amp;rsquo;s settings.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As starting point for this I used &lt;a href=&#34;https://github.com/carlossg/jenkins-kubernetes-plugin&#34;&gt;this repo from carlossg&lt;/a&gt; and &lt;a href=&#34;https://github.com/jenkinsci/kubernetes-plugin&#34;&gt;the official documentation on the kubernetes-plugin from Jenkins&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Of course it didn&amp;rsquo;t really work without a lot of trial and error, hence the reason for writing this article.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After clicking &lt;code&gt;Start using Jenkins&lt;/code&gt;, you are ready to Enable Proxy Compatibility from Configure Jenkins &amp;ndash;&amp;gt; Configure Global Security &amp;ndash;&amp;gt; Enable Proxy Compatibility - of course, not after a lot of tries and failure because &lt;code&gt;No valid crumb was included in request&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;configuring-jenkins-to-dinamically-spawn-agents-docker-containers-for-builds&#34;&gt;Configuring Jenkins to dinamically spawn agents (Docker containers) for builds&lt;/h2&gt;

&lt;p&gt;Since we want to have dynamically spawned agents (or slaves), we will keep 0 executors (from node settings):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/executors.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then, we need to install the &lt;a href=&#34;https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin&#34;&gt;Kubernetes Plugin for Jenkins&lt;/a&gt; - Manage Jenkins &amp;ndash;&amp;gt; Manage Plugins &amp;ndash;&amp;gt; Available &amp;ndash;&amp;gt; Search for &lt;code&gt;kubernetes plugin&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/kubernetes-plugin.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then restart Jenkins after installing the plugin.&lt;/p&gt;

&lt;p&gt;Now you need to configure the Kubernetes plugin: Manage Jenkins &amp;ndash;&amp;gt; Configure System, and all the way to the bottom &amp;ndash;&amp;gt; Add a new cloud.&lt;/p&gt;

&lt;p&gt;Add the credentials to your Kubernetes cluster as in the picture below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/kubernetes-credentials.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then add the Kubernetes master FQDN (Fully Qualified Domain Name) and test the connection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/test-connection.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then add the Jenkins URL (with http!) and the Jenkins tunnel (without http and with the 50000 port!) and set a resonable container cap (how many containers should run at the same time):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/jenkins-url-tunnel.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The other settings in the picture I left untouched.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next we need to configure a container templat based on which Jenkins will spawn slaves to execute builds. This part is rather tricky and pretty undocumented, so it took me quite a lot. Here we go:&lt;/p&gt;

&lt;p&gt;We need to Add a Pod Template &amp;ndash;&amp;gt; Kubernetes Pod Template. The name of the template should be &lt;code&gt;jnlp&lt;/code&gt;, otherwise this will not work.&lt;/p&gt;

&lt;p&gt;Next we need to add a container template. The name of the container should also be &lt;code&gt;jnlp&lt;/code&gt;, or it will not work.&lt;/p&gt;

&lt;h2 id=&#34;the-docker-image-for-the-slaves&#34;&gt;The Docker image for the slaves&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/jenkinsci/kubernetes-plugin&#34;&gt;As the documentation states&lt;/a&gt;, you need to use a &lt;a href=&#34;https://hub.docker.com/r/jenkinsci/jnlp-slave/&#34;&gt;jnlp-slave image from Docker Hub&lt;/a&gt;. If we take a look at the Dockerfile for this image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM jenkinsci/slave:alpine
MAINTAINER Nicolas De Loof &amp;lt;nicolas.deloof@gmail.com&amp;gt;

COPY jenkins-slave /usr/local/bin/jenkins-slave

ENTRYPOINT [&amp;quot;jenkins-slave&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will surely work, but all this is going to do is give you a container based on the openjdk container (so with a Java SDK) and git.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/jenkinsci/slave/~/dockerfile/&#34;&gt;Dockerfile for the base image for the jnlp-slave&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But we might have more and diverse workloads - and since we can configure the slave container only globally, we might just install more frameworks in this container - Node, Ruby, .NET Core - but this is just against what containerization stands for. We want to be able to build any kind of containerized workload with our Jenkins instance.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what we want to do:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- be able to build any kind of workload on our Jenkins isntance
- deploy the built images / applications back to Kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So clearly we need to update our Dockerfile by installing the Docker client and the kubectl CLI.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s have a look at how the Dockerfile would look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM jenkinsci/slave:alpine

USER root
RUN apk add --no-cache \
ca-certificates \
curl \
openssl

ENV DOCKER_BUCKET get.docker.com
ENV DOCKER_VERSION 17.04.0-ce
ENV DOCKER_SHA256 c52cff62c4368a978b52e3d03819054d87bcd00d15514934ce2e0e09b99dd100

RUN set -x \
&amp;amp;&amp;amp; curl -fSL &amp;quot;https://${DOCKER_BUCKET}/builds/Linux/x86_64/docker-${DOCKER_VERSION}.tgz&amp;quot; -o docker.tgz \
&amp;amp;&amp;amp; echo &amp;quot;${DOCKER_SHA256} *docker.tgz&amp;quot; | sha256sum -c - \
&amp;amp;&amp;amp; tar -xzvf docker.tgz \
&amp;amp;&amp;amp; mv docker/* /usr/local/bin/ \
&amp;amp;&amp;amp; rmdir docker \
&amp;amp;&amp;amp; rm docker.tgz \
&amp;amp;&amp;amp; docker -v

RUN curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl

RUN chmod +x ./kubectl
RUN mv ./kubectl /usr/local/bin/kubectl

COPY docker-entrypoint.sh /usr/local/bin/

COPY jenkins-slave /usr/local/bin/jenkins-slave

RUN chmod +x /usr/local/bin/docker-entrypoint.sh
RUN chmod +x /usr/local/bin/jenkins-slave

ENTRYPOINT docker-entrypoint.sh; jenkins-slave

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It starts from the same base image as the Jenkins &lt;code&gt;jnlp-slave&lt;/code&gt;, but it also adds Docker and kubectl. I built and pushed this image to &lt;a href=&#34;https://hub.docker.com/r/radumatei/jenkins-slave-docker/&#34;&gt;radumatei/jnlp-slave-docker:kubectl&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So we can use this as the base image for the slave containers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/settings.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since we want to mount the &lt;code&gt;/var/run/docker.sock&lt;/code&gt; from the host to the container so the containers can use the Docker engine installed on the node.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The solution is &lt;a href=&#34;http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/&#34;&gt;based on this article from Jérôme Petazzoni&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Remember that in our Dockerfile we also installed kubectl. Since we need to actually modify deployments in the cluster using kubectl from inside the contaier, we need to authenticate the container in some way. Since the node that runs the slave pod that is doing the build is actually part of the cluster, this means at &lt;code&gt;/root/.kube&lt;/code&gt; there should be the information about the cluster which would allow us to actually make a deployment against the cluster. So we also mount &lt;code&gt;/root/.kube&lt;/code&gt; from the host to the container.&lt;/p&gt;

&lt;p&gt;This should be pretty much all setup involved. It might not sound much, but it is rather undocumented or not up to date.&lt;/p&gt;

&lt;p&gt;Before we setup the pipeline, let&amp;rsquo;s create a public service on Kubernetes that we will update after a build.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/public-service.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While configuring the pipeline, you also need the Docker Hub credentials:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/docker-hub-credentials.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So setup your own GitHub project (that has a Dockerfile at the root of the project) as a Jenkins pipeline with the following configuration:&lt;/p&gt;

&lt;p&gt;STEP 1:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build -t ${DOCKER_HUB_USER}/dotnet-core-kubernetes:${BUILD_NUMBER} .
docker login -u ${DOCKER_HUB_USER} -p ${DOCKER_HUB_PASSWORD}
docker push ${DOCKER_HUB_USER}/dotnet-core-kubernetes:${BUILD_NUMBER}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;STEP 2:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl set image deployment/dotnet-core-kubernetes dotnet-core-kubernetes=${DOCKER_HUB_USER}/dotnet-core-kubernetes:${BUILD_NUMBER}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you configured the webhook correctly in GitHub, with every commit on the branches you specified should trigger a build for Jenkins:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/build.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then, the kubectl command will actually update your application on the cluster.&lt;/p&gt;

&lt;h2 id=&#34;what-is-happening-behind-the-scenes&#34;&gt;What is happening behind the scenes?&lt;/h2&gt;

&lt;p&gt;With every initiated build, the Jenkins master will start a new pod in the Kubernetes cluster based on the Docker image you specified when configuring it (in this case &lt;a href=&#34;https://hub.docker.com/r/radumatei/jenkins-slave-docker/&#34;&gt;radumatei/jnlp-slave-docker:kubectl&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/creating.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It will pull that image and once the application starts, it will connect to port 50000 on the Jenkins service to register as available to serve builds.&lt;/p&gt;

&lt;p&gt;It will start the build steps on this container and you can see this in the logs from the build:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/build1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/build2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After the build finishes (regardless of success or failure), the master will terminate the slave and all resources will be released:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/kubernetes-jenkins-azure/spike.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article I tried to clearly show how to get started with a Jenkins master as a Kubernetes service that dynamically creates slaves to execute your build and then terminates them.&lt;/p&gt;

&lt;p&gt;The great advantage of this is that you only start containers once they have a task (in this case a build) to execute.&lt;/p&gt;

&lt;p&gt;We managed to build a complete workflow with Kubernetes and Jenkins that will automatically build and integrate your updates to your application.&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;

&lt;p&gt;As a next step I will investigate &lt;a href=&#34;https://github.com/kubernetes/helm&#34;&gt;deploying Jenkins using Helm&lt;/a&gt;, which I assume should be a much simpler task.&lt;/p&gt;

&lt;h2 id=&#34;feedback&#34;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;If you stumbled upon this article, please take a minute to provide feedback to it - did it help, do you know a better or simpler way of achieving this?&lt;/p&gt;

&lt;p&gt;Your feedback is highly appreciated!&lt;/p&gt;

&lt;h2 id=&#34;thanks-for-reading&#34;&gt;Thanks for reading! :)&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Dockerizing an ASP.NET Core application with GitHub, Docker Cloud and Azure</title>
      <link>http://radu-matei.github.io/blog/aspnet-core-docker-azure/</link>
      <pubDate>Sat, 26 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://radu-matei.github.io/blog/aspnet-core-docker-azure/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this article, we will take the simplest ASP.NET Core application, run it with Docker locally, then create Continuous Integration and Continuous Deployment flows using a GitHub repository, Docker Cloud and an Azure virtual machine that will act as a node for Docker Cloud.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t want to create an ASP.NET Core application but are interested in the CI/CD workflow, or if you already have a GitHub repository with a complete application with a Dockerfile, &lt;a href=&#34;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&#34;&gt;you might want to skip to the part we start creating the CI/CD workflow.&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;moving-parts-and-used-components&#34;&gt;Moving parts and used components&lt;/h2&gt;

&lt;p&gt;The main part of a CI/CD workflow like this is the application itself. It can be however complicated, but in this case I want to emphasize the workflow itself and will only build a very simple application with ASP.NET Core.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you can use this article with any single-container application you want to build.&lt;/p&gt;

&lt;p&gt;However, if you want to build multi-container applications, you will most likely need a way to compose and orchestrate those containers. In future articles, we will also deal with multi-container applications, but in this one we will keep things easy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will create a GitHub repository that we will use to create a Docker image and push it to Docker Hub.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker Hub is a cloud-based registry service which allows you to link to code repositories, build your images and test them, stores manually pushed images, and links to Docker Cloud so you can deploy images to your hosts. It provides a centralized resource for container image discovery, distribution and change management, user and team collaboration, and workflow automation throughout the development pipeline.&lt;/p&gt;

&lt;p&gt;More information about Docker Hub on &lt;a href=&#34;https://docs.docker.com/docker-hub/&#34;&gt;the Official Docker Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then, we will configure an Azure VM to be a node for Docker Cloud and Docker Cloud will automatically publish containers to that VM. Then, every time there are changes in the GitHub repository, Docker Cloud will build the image and publish the container again automatically.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;More information about Docker Cloud on &lt;a href=&#34;https://docs.docker.com/docker-cloud/overview/&#34;&gt;the Official Docker Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/ci-cd-workflow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.docker.com/2016/04/cicd-with-docker-cloud/&#34;&gt;Photo source on the Docker Blog&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;create-a-github-repository&#34;&gt;Create a GitHub repository&lt;/h2&gt;

&lt;p&gt;First, we need a GitHub repository. If you already have a repo with an application you want to use you can do that. However, I will create a new repo and clone it on my computer.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that you can follow this article regardless of your computer OS. It can be done with Windows, Linux or macOS.&lt;/p&gt;

&lt;p&gt;In creating this article, I used macOS, with Docker for Mac and Visual Studio Code.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/github-new-repo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since this is a .NET Core application, I chose to add a &lt;code&gt;.gitignore&lt;/code&gt; file that will ignore all .NET specific output files after building the application.&lt;/p&gt;

&lt;p&gt;Create the repository, then clone it somewhere locally on your computer. In my case, I would execute &lt;code&gt;git clone https://github.com/radu-matei/aspnet-core-docker-azure&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-asp-net-core-application&#34;&gt;Creating the ASP.NET Core application&lt;/h2&gt;

&lt;p&gt;This will be the part with the least focus in this article, since we have covered building ASP.NET Core applications for a while now and you can find a lot resources on this topic, including some on this site.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For ASP.NET Core tutorials, you can &lt;a href=&#34;https://radu-matei.github.io/categories/aspnet-core/&#34;&gt;take a look at some resources on this blog&lt;/a&gt;, consult the &lt;a href=&#34;https://docs.microsoft.com/en-us/aspnet/core/&#34;&gt;official documentation&lt;/a&gt;, or you can watch this &lt;a href=&#34;https://mva.microsoft.com/en-US/training-courses/introduction-to-aspnet-core-10-16841&#34;&gt;Microsoft Virtual Academy course presented by Scott Hanselman and Maria Naggaga&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, we will create the same application as explained in &lt;a href=&#34;https://radu-matei.github.io/blog/aspnet-core-startup/&#34;&gt;this blog post&lt;/a&gt;, but we will build it against .NET Core 1.0.1 (which is the latest stable version at the moment of writing this article).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;While .NET Core 1.0.1 is the latest version at the moment of writing this article, you can also use other versions, since the Docker images are available on Docker Hub.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the folder that was just created from cloning the repository, execute &lt;code&gt;dotnet new&lt;/code&gt; in order to create a new .NET Core application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/git-clone-dotnet.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now depending on the .NET Core version you have installed on your machine, &lt;code&gt;project.json&lt;/code&gt; will look slightly different:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Microsoft.NETCore.App&amp;quot;: {
  &amp;quot;type&amp;quot;: &amp;quot;platform&amp;quot;,
  &amp;quot;version&amp;quot;: &amp;quot;1.0.1&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since &lt;code&gt;1.0.1&lt;/code&gt; is the latest stable version, we will use it as example for this application.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can use any version available on your machine and as image from Microsoft on Docker Hub.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Add the required Kestrel dependency in &lt;code&gt;project.json&lt;/code&gt;, keeping in mind that the version is &lt;code&gt;1.0.1&lt;/code&gt; and respond to any incoming request with a message and the current date and time of the server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    public static void Main(string[] args)
        {
            var host = new WebHostBuilder()
                .UseKestrel()
                .Configure(app =&amp;gt; app.Run(context =&amp;gt; 
                {
                    return context.Response.WriteAsync($&amp;quot;Hello, Universe! It is {DateTime.Now}&amp;quot;);
                }))
                .Build();

            host.Run();
        }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is the entire ASP.NET Core application we will use for this article.&lt;/p&gt;

&lt;h2 id=&#34;writing-the-dockerfile&#34;&gt;Writing the Dockerfile&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34;&gt;More information on the Dockerfile on the Official Docker Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Basically, the Dockerfile is like a recipe for building container images. It is a script composed of multiple commands executed succesively to create images based on other images.&lt;/p&gt;

&lt;p&gt;You have two options for writing the Dockerfile: you can write it manually, or you can have VS Code write it for you. If you &lt;a href=&#34;https://code.visualstudio.com/Docs/languages/dockerfile&#34;&gt;install the VS Code Docker extension&lt;/a&gt;, press F1 and search for Docker, you should see something similar to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/vscode-docker.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, we will write the Dockerfile manually, mainly because we want to understand all the things involved.&lt;/p&gt;

&lt;p&gt;Create a new file called &lt;code&gt;Dockerfile&lt;/code&gt; (without extension) to the root of the application (in this case in the same folder as &lt;code&gt;project.json&lt;/code&gt; and &lt;code&gt;Program.cs&lt;/code&gt;) with the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM microsoft/dotnet:1.0.1-sdk-projectjson

COPY . /app
WORKDIR /app

RUN [&amp;quot;dotnet&amp;quot;, &amp;quot;restore&amp;quot;]
RUN [&amp;quot;dotnet&amp;quot;, &amp;quot;build&amp;quot;]

EXPOSE 5000/tcp
ENV ASPNETCORE_URLS http://*:5000

ENTRYPOINT [&amp;quot;dotnet&amp;quot;, &amp;quot;run&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The content of the Dockerfile is pretty self-explanatory:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it gets a base image that has &lt;code&gt;dotnet&lt;/code&gt; installed, the &lt;code&gt;microsoft/dotnet:1.0.1-sdk-projectjson&lt;/code&gt; image&lt;/li&gt;
&lt;li&gt;it copies the source of the application inside the container, in the &lt;code&gt;/app&lt;/code&gt; folder&lt;/li&gt;
&lt;li&gt;it sets the &lt;code&gt;/app&lt;/code&gt; folder as the working folder where the commands will be executed from&lt;/li&gt;
&lt;li&gt;executes &lt;code&gt;dotnet restore&lt;/code&gt; and &lt;code&gt;dotnet build&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;expoes the 5000 port&lt;/li&gt;
&lt;li&gt;sets the environment variable for ASP .NET Core in the container&lt;/li&gt;
&lt;li&gt;when the container starts it will execute the &lt;code&gt;dotnet run&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;In this case, we both build and run the application inside the container. In a production environment, we would only use the &lt;code&gt;dotnet runtime&lt;/code&gt; image from Microsoft that is only able to execute applications and not build them. This would result in a much smaller footprint of the image.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;building-the-image&#34;&gt;Building the image&lt;/h2&gt;

&lt;p&gt;At this point, we have configured the application (which wasn&amp;rsquo;t that hard), we have a definition for Docker, our Dockerfile, but we haven&amp;rsquo;t built an image or a container so far.&lt;/p&gt;

&lt;p&gt;The end result is for us to start a container. Every container is built upon an image, that is composed of the application itself and its dependencies.&lt;/p&gt;

&lt;p&gt;To build the image, simply run the following command in the same folder with the Dockerfile:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker build -t aspnet-core-docker-azure&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-build-1.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-build-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can clearly see how each step in the Dockerfile is executed succesively and how at every step an intermediate container gets created. This is done so that if the execution fails at let&amp;rsquo;s say STEP 7, all progress made up to that point doesn&amp;rsquo;t get lost. After every successful step executed, the previous container is removed.&lt;/p&gt;

&lt;p&gt;Running &lt;code&gt;docker images&lt;/code&gt; should show you the newly created image containing your application and its dependencies (among other images that you might have).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-images.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Notice though that the base for our image also got pulled from Docker Hub - &lt;code&gt;microsoft/dotnet:1.0.1-sdk-projectjson&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;running-a-new-container&#34;&gt;Running a new container&lt;/h2&gt;

&lt;p&gt;Now that we built our image it&amp;rsquo;s time to run a new container based on that image.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;docker run -d -p 8080:5000 -t aspnet-core-docker-azure&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s examine the aruments passed along the &lt;code&gt;docker run&lt;/code&gt; command:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;-d&lt;/code&gt; - the container will run in &lt;code&gt;detached&lt;/code&gt; mode, so we won&amp;rsquo;t see logs from the container as output&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;-p 8080:5000&lt;/code&gt; - this will map the 5000 port inside the container (that the application is running on&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;remember the Dockerfile) to port 8080 from the host&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;code&gt;-t&lt;/code&gt; - the tag of the image this container is based on&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This command started our container, so Docker must have executed &lt;code&gt;dotnet run&lt;/code&gt; inside the container (remember the last line in the Dockerfile), so the application should have started.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-run.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The output of this command is the id of the newly created container, so we can verify that the container is running using the &lt;code&gt;docker ps&lt;/code&gt; command:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-ps.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can see the id of the container, the image it is based on, the command used as entrypoint and the port mapping: 8080 on the host to 5000 inside the container.&lt;/p&gt;

&lt;p&gt;So if we navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; we should see our application running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-run-browser.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So far we created a very simple ASP .NET Core application and we ran it locally inside Docker.We  haven&amp;rsquo;t used the GitHub repo, Docker Hub, Docker Cloud or Azure just yet. This is where we start doing so.&lt;/p&gt;

&lt;h2 id=&#34;setup-an-azure-vm-as-node-for-docker-cloud&#34;&gt;Setup an Azure VM as node for Docker Cloud&lt;/h2&gt;

&lt;p&gt;While Docker Cloud allows you to run containers and build images on some free tier servers, you would most likely want to do it on your own machine.&lt;/p&gt;

&lt;p&gt;If you link the Docker Cloud account with your cloud subscription (in this case Azure), you can create nodes and clusters directly from the Docker Cloud portal.&lt;/p&gt;

&lt;p&gt;In this case we will normally create a VM from the Azure Portal (or from any other cloud provider or on-premise) and install the Docker Cloud agent.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-cloud-bring-node.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I created an Ubuntu Server 14.04 VM (at the moment of writing this article, only Ubuntu 14.04 and 15.04 are supported by Docker Cloud).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/azure-create-vm-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After the deployment succeeds, we will need to open some ports on that VM so the Docker Cloud self discovery service can work. &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-nsg-quickstart-portal&#34;&gt;In this article you can see the detalied process on how to open ports for Azure VMs.&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;We recommend you open incoming port 2375 in your firewall for Docker Cloud to communicate with the Docker daemon running in the node. For the overlay network to work, you must open port 6783/tcp and 6783/udp.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You have to find the &lt;code&gt;Network Security Group&lt;/code&gt; tab from the VM settings, then the &lt;code&gt;Network Security Group&lt;/code&gt; tab then the &lt;code&gt;Inbound Security Roules&lt;/code&gt; tab.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/azure-network-security.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the Docker Cloud documentation states, we should open ports 2375 and 6783/tcp and udp.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-2375.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then do the same for 6783/tcp and 6783/udp, and since this VM will host the running container, I will also open a port for HTTP - which will automatically open port 80.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you plan to run multiple containers at the same time that expose ports on this machine, you should open more ports to be accessible from outside the VM.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I will also setup a DNS name for the VM so that I don&amp;rsquo;t have to remember the IP of the machine:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/azure-vm-dns.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you should be able to SSH into the machine and install the Docker Cloud agent.&lt;/p&gt;

&lt;p&gt;On macOS, Linux or Bash on Windows, to SSH into a machine:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssh user-name@your-machine-dns-or-ip&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In my case, I would run
 &lt;code&gt;ssh radu-matei@ubuntu-docker-cloud.westeurope.cloudapp.azure.com&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/azure-vm-ssh.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After this, I could just paste the command that installs the Docker Cloud agent:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl -Ls https://get.cloud.docker.com/ | sudo -H sh -s your-unique-hash&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You might still see some Tutum references in the scripts, as this was the name of the company acquired by Docker that initially developed the functionality behind Docker Cloud.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After the command above successfully executed and you refreshed your Docker Cloud tab, you should see your newly created node.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-node.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is all the required setup for a VM to be a Docker Cloud node.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-repository-in-docker-cloud&#34;&gt;Creating a repository in Docker Cloud&lt;/h2&gt;

&lt;p&gt;By now, the GitHub repository with the application should be up to date, since we will use it to create a new Docker Cloud repository that will automatically build images on every git push in the GitHub repo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-create-repo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;By default, the webhook will be setup for the master branch on every push, but you can also set it up for specific events, like certain releases.&lt;/p&gt;

&lt;p&gt;By default, the newly created image will be public, but you can make it private. Note that you have a limited number of private repositories in the free tier.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-repo-info.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After it was created and Docker Cloud successfully tested the connection with GitHub, we should set the build process so that it uses the node we just provided and not the shared one provided by Docker Cloud:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/docker-repo-build.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you configured automated notifications on Slack, every time there is an event related to the service you are creating, you will have notifications on Slack.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/slack-notifications.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After you click &lt;code&gt;Save and Build&lt;/code&gt; the image building will start on the machine you provided.&lt;/p&gt;

&lt;p&gt;At any time you can see the logs from building the image in the &lt;code&gt;Builds&lt;/code&gt; tab.
&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/build-logs.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now if you go to Docker Hub you should see your newly created image.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-service-based-on-the-image-we-created&#34;&gt;Creating a service based on the image we created&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;A service is a group of containers of the same image:tag. Services make it simple to scale your application. With Docker Cloud, you simply drag a slider to change the number of containers in a service.&lt;/p&gt;

&lt;p&gt;Before you can deploy a service in Docker Cloud, you must have at least one node deployed. If you haven’t done this yet follow the tutorial to deploy a node .&lt;/p&gt;

&lt;p&gt;When you create a service in the Docker Cloud web interface, a wizard walks you through configuring the service in three steps.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Choose a Container Image Images can come from Docker Cloud’s Jumpstarts library, your personal Docker Hub account or Docker Hub’s public index, or from third party registries you connect.&lt;/li&gt;
&lt;li&gt;Configure the Service From here, give the service a name, set the initial number of containers, expose/publish ports, modify the run command or entrypoint, set memory and CPU limits.&lt;/li&gt;
&lt;li&gt;Set Environment variables Set the edit environment variables and link your service to other existing services in Docker Cloud.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;More on Docker Cloud services on &lt;a href=&#34;https://docs.docker.com/docker-cloud/getting-started/your_first_service/&#34;&gt;the Official Docker Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will create a service based on the image we just created.&lt;/p&gt;

&lt;p&gt;The only custom settings will be to enable the &lt;code&gt;AUTOREDEPLOY&lt;/code&gt; option and to specify the port to be 80 on the machine.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/autoredeploy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/port.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After hitting the create, it will create the service and already start a container based on this service.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/service-starting.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we go to the containers tab, we can see the container running.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/containers.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;testing-the-application&#34;&gt;Testing the application&lt;/h2&gt;

&lt;p&gt;Remember the DNS we assigned to the Azure VM? In my case it was &lt;code&gt;http://ubuntu-docker-cloud.westeurope.cloudapp.azure.com/&lt;/code&gt;. Normally, the container should have started on port 80 (the default HTTP port) on this machine.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s try and access that exact URL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/public-app.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At this point, you can create additional service and start containers on this machine, provided you open ports on the VM with the procedure described above.&lt;/p&gt;

&lt;h2 id=&#34;updating-the-application&#34;&gt;Updating the application&lt;/h2&gt;

&lt;p&gt;Because we setup the image based on the GitHub repository and we checked the &lt;code&gt;AUTOREDEPLOY&lt;/code&gt; option, every time we will push on the master branch of the repository, the entire system will update itself.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s slightly modify the application and push the modifications. This should trigger the auto build and auto redeploy of the container and without us doing anything, the modifications should be live.&lt;/p&gt;

&lt;p&gt;I just changed the message the application responds with and pushed the modifications to the master branch. This should trigger the build and redeploy of the container.&lt;/p&gt;

&lt;p&gt;You should see the new build in the &lt;code&gt;Recent Builds&lt;/code&gt; tab from the &lt;code&gt;Repositories&lt;/code&gt; page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/recent-builds.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can also see all events in Slack:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/slack-build-events.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After the build and redeploy are successful, accessing the application should reflect the modifications:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://radu-matei.github.io/img/article-photos/aspnet-core-docker-azure/public-app-updated.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This is basically how the entire process looks like. It is not production ready, as it does not have any testing workflow put in place and the application is rather simple.&lt;/p&gt;

&lt;p&gt;Real world scenarions would most certainly involve more containers, so composing and orchestrating containers, as well as testing.&lt;/p&gt;

&lt;p&gt;We will try to deal with these aspects in future articles, but for now we created a very simple CI/CD workflow using GitHub, Docker Cloud and an Azure VM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ASP.NET Core MVC and SignalR Core</title>
      <link>http://radu-matei.github.io/blog/aspnet-core-mvc-signalr/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://radu-matei.github.io/blog/aspnet-core-mvc-signalr/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this article we will take a look at how to integrate ASP.NET Core MVC with SignalR Core (at the moment of writing this article, the latest version of SignalR is &lt;a href=&#34;https://dotnet.myget.org/feed/aspnetcore-ci-dev/package/nuget/Microsoft.AspNetCore.SignalR.Server&#34;&gt;&lt;code&gt;0.2.0-alpha1-22107&lt;/code&gt;&lt;/a&gt;) and how to use the SignalR context outside hubs (and solve the current issues with the custom resolvers that will be detailed later) to update clients.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This article assumes a basic understanding of ASP.NET Core MVC and will not try to explain all concepts here. &lt;a href=&#34;https://radu-matei.github.io/blog/aspnet-core-api/&#34;&gt;This article&lt;/a&gt; does an introduction to ASP.NET Core MVC and has the basic application structure needed for this article.&lt;/p&gt;

&lt;p&gt;In order to get started with SignalR Core, &lt;a href=&#34;https://radu-matei.github.io/blog/signalr-core/&#34;&gt;take a look at this article&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will build on &lt;a href=&#34;https://radu-matei.github.io/blog/aspnet-core-api/&#34;&gt;this application&lt;/a&gt; and add real time functionality to it.&lt;/p&gt;

&lt;p&gt;It is a very simple application that will enable the creation of posts (much like messages) and it took us through adding the MVC services, creating models, controllers and consuming some data. Now, we want all connected users to see in real time when somebody adds posts in the page without refreshing it.&lt;/p&gt;

&lt;h2 id=&#34;configure-the-application&#34;&gt;Configure the application&lt;/h2&gt;

&lt;p&gt;NuGet has the ability to get packages from more than one source, and we need it to download alpha packages that are only available on the MyGet server of the ASP.NET team, besides the ASP.NET Core packages available on NuGet. So in the root of the application add a file called &lt;code&gt;NuGet.Config&lt;/code&gt; with the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;configuration&amp;gt;
    &amp;lt;packageSources&amp;gt;
        &amp;lt;clear/&amp;gt;
            &amp;lt;add key=&amp;quot;aspnetcidev&amp;quot; value=&amp;quot;https://dotnet.myget.org/F/aspnetcore-ci-dev/api/v3/index.json&amp;quot;/&amp;gt;
            &amp;lt;add key=&amp;quot;api.nuget.org&amp;quot; value=&amp;quot;https://api.nuget.org/v3/index.json&amp;quot;/&amp;gt;
    &amp;lt;/packageSources&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;add-the-required-packages&#34;&gt;Add the required packages&lt;/h2&gt;

&lt;p&gt;For this application, we will need the Kestrel web server (of course), the MVC packages so we can use controllers, static files so we can have HTML and JavaScript files served, SignalR and WebSockets:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;Microsoft.AspNetCore.Server.Kestrel&amp;quot;: &amp;quot;1.0.0&amp;quot;,
&amp;quot;Microsoft.AspNetCore.Mvc&amp;quot;: &amp;quot;1.0.0&amp;quot;,
&amp;quot;Microsoft.AspNetCore.StaticFiles&amp;quot;: &amp;quot;1.0.0&amp;quot;,
&amp;quot;Microsoft.AspNetCore.SignalR.Server&amp;quot;: &amp;quot;0.2.0-*&amp;quot;,
&amp;quot;Microsoft.AspNetCore.WebSockets&amp;quot;: &amp;quot;0.2.0-*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;program-cs&#34;&gt;&lt;code&gt;Program.cs&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;Besides adding the &lt;code&gt;UseContentRoot&lt;/code&gt; statement, the &lt;code&gt;Main&lt;/code&gt; method is pretty much standard for an ASP.NET Core application.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; public static void Main(string[] args)
 {
     var host = new WebHostBuilder()
         .UseKestrel()
         .UseContentRoot(Directory.GetCurrentDirectory())
         .UseStartup&amp;lt;Startup&amp;gt;()
         .Build();

     host.Run();
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;post-ipostrepository-and-postrepository&#34;&gt;&lt;code&gt;Post&lt;/code&gt;, &lt;code&gt;IPostRepository&lt;/code&gt; and &lt;code&gt;PostRepository&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://radu-matei.github.io/blog/aspnet-core-api/&#34;&gt;This article&lt;/a&gt; explains all these components thoroughly, but let&amp;rsquo;s see briefly what each one does:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;public class Post
{
    public int Id { get; set; }
    public string UserName { get; set; }
    public string Text { get; set; }

    public Post(int id, string userName, string text)
    {
        Id = id;
        UserName = userName;
        Text = text;
    }

    public Post()
    {
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;Post&lt;/code&gt; will be the model that users add in the application. It is a straightforward C# class with three auto-implemented properties and two constructors.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Each user that enters can publish a post containing his user name and a text, so our Post class only contains two properties for the UserName and Text of the post and an Id.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Generic;

public interface IPostRepository
{
    List&amp;lt;Post&amp;gt; GetAll();
    Post GetPost(int id);
    void AddPost(Post post);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Regardless of where that data is going to be stored, there should be a consistent way of reading and writing, and we will achieve this through an interface, IPostRepository, that will expose the minimum necessary methods: a method to read all posts, a method to add a post and a method to retrieve a post with a specified id.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Generic;
using System.Linq;

public class PostRepository : IPostRepository
{
    private List&amp;lt;Post&amp;gt; _posts = new List&amp;lt;Post&amp;gt;()
    {
        new Post(1, &amp;quot;Obi-Wan Kenobi&amp;quot;,&amp;quot;These are not the droids you&#39;re looking for&amp;quot;),
        new Post(2, &amp;quot;Darth Vader&amp;quot;,&amp;quot;I find your lack of faith disturbing&amp;quot;)
    };
    public void AddPost(Post post)
    {
        _posts.Add(post);
    }

    public List&amp;lt;Post&amp;gt; GetAll()
    {
        return _posts;
    }

    public Post GetPost(int id)
    {
        return _posts.FirstOrDefault(p =&amp;gt; p.Id == id);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;We implemented the &lt;code&gt;IPostRepository&lt;/code&gt; interface through an in-memory class called &lt;code&gt;PostRepository&lt;/code&gt; that holds the data in a list. Since we have the three methods to access the data, there is no need to expose the post list outside the class, so it is be private. Besides the list, we only need to implement the three methods from the interface.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;the-postscontroller&#34;&gt;The &lt;code&gt;PostsController&lt;/code&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Generic;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.SignalR.Infrastructure;

public class PostsController : Controller
{
    private IPostRepository _postRepository { get; set; }

    public PostsController(IPostRepository postRepository)
    {
        _postRepository = postRepository;
    }

    [HttpGet]
    public List&amp;lt;Post&amp;gt; GetPosts()
    {
        return _postRepository.GetAll();
    }

    [HttpGet]
    public Post GetPost(int id)
    {
        return _postRepository.GetPost(id);
    }

    [HttpPost]
    public void AddPost(Post post)
    {
        _postRepository.AddPost(post);
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this controller, we will update all clients when a post was added using SignalR by getting the context of the hub and calling client methods  using a &lt;code&gt;ConnectionManager&lt;/code&gt;. We will see later how to achieve this.&lt;/p&gt;

&lt;h2 id=&#34;add-a-hub&#34;&gt;Add a hub&lt;/h2&gt;

&lt;p&gt;In this application, we will use the hub as a proxy: clients will not call hub methods directly. SignalR will be used to provide server updates - that is the server notifies all clients that something happened, so the clients must first connect to a hub.&lt;/p&gt;

&lt;p&gt;So the hub class will be incredibly simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using Microsoft.AspNetCore.SignalR;

public class PostsHub : Hub
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will see how to use the hub context outside of the hub a little later.&lt;/p&gt;

&lt;h2 id=&#34;camel-case-issues-and-custom-contract-resolvers&#34;&gt;Camel case issues and custom contract resolvers&lt;/h2&gt;

&lt;p&gt;At this moment, MVC uses camel case notation to pass JSON to clients. That means that even if on the server you write your class properties with Pascal case notation (as you should!), JavaScript clients would get Camel cased objects.&lt;/p&gt;

&lt;p&gt;This behavior is new to ASP.NET Core and was not present in the older versions when SignalR was used and built, so SignalR and its clients all expect Pascal case objects, while the objects passed between MVC and its clients are camel cased. This means we cannot reuse metods across the JavaScript client, thing we cannot tolerate.&lt;/p&gt;

&lt;p&gt;So we are going to make SignalR pass objects in camel case. Basically, we are going to &amp;ldquo;recycle&amp;rdquo; &lt;a href=&#34;https://github.com/SignalR/SignalR/issues/500#issuecomment-7453751&#34;&gt;this old SignalR GitHub issue&lt;/a&gt; and adapt it to our versions of ASP.NET and SignalR.&lt;/p&gt;

&lt;h2 id=&#34;add-a-custom-contract-resolvers&#34;&gt;Add a custom contract resolvers&lt;/h2&gt;

&lt;p&gt;A first try could be to change the default contract resolver to a &lt;code&gt;CamelCasePropertyNamesContractResolver()&lt;/code&gt; inside the &lt;code&gt;ConfigureServices&lt;/code&gt; method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var settings = new JsonSerializerSettings();
settings.ContractResolver = new CamelCasePropertyNamesContractResolver();

var serializer = JsonSerializer.Create(settings);
services.Add(new ServiceDescriptor(typeof(JsonSerializer), 
             provider =&amp;gt; serializer, 
             ServiceLifetime.Transient));

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While this is not far from what we are going to do, at this moment we simply cannot force all components to pass camel case objects because it would break current conventions in place.&lt;/p&gt;

&lt;p&gt;Simply put, if you try this the JavaScript client will no longer connect, because all connection and internal communication is tansformed to camel case.&lt;/p&gt;

&lt;p&gt;We are at the point where we need all of the application objects to be passed camel cased, and all connection and SignalR internal objects to be unmodified.&lt;/p&gt;

&lt;p&gt;We will write a custom contract resolver that looks at the assembly of the object type and if it is not an internal SignalR object (if it is not from the same assembly as &lt;code&gt;Connection&lt;/code&gt;, a class from SignalR), then it modifies it to be camel case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;using System;
using System.Reflection;
using Microsoft.AspNetCore.SignalR.Infrastructure;
using Newtonsoft.Json.Serialization;

    public class SignalRContractResolver : IContractResolver
    {
        private readonly Assembly _assembly;
        private readonly IContractResolver _camelCaseContractResolver;
        private readonly IContractResolver _defaultContractSerializer;

        public SignalRContractResolver()
        {
            _defaultContractSerializer = new DefaultContractResolver();
            _camelCaseContractResolver = new CamelCasePropertyNamesContractResolver();
            _assembly = typeof(Connection).GetTypeInfo().Assembly;
        }


        public JsonContract ResolveContract(Type type)
        {
            if (type.GetTypeInfo().Assembly.Equals(_assembly))
                return _defaultContractSerializer.ResolveContract(type);

            return _camelCaseContractResolver.ResolveContract(type);
        }

    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So in &lt;code&gt;ConfigureServices&lt;/code&gt; we register our contract resolver very similarly to what we had earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var settings = new JsonSerializerSettings();
settings.ContractResolver = new SignalRContractResolver();

var serializer = JsonSerializer.Create(settings);
services.Add(new ServiceDescriptor(typeof(JsonSerializer), 
             provider =&amp;gt; serializer, 
             ServiceLifetime.Transient));
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that most likely, this will be done more elegantly in future releases.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;the-configure-method&#34;&gt;The &lt;code&gt;Configure&lt;/code&gt; method&lt;/h2&gt;

&lt;p&gt;Again, the &lt;code&gt;Configure&lt;/code&gt; method is pretty straightforward: we add static files support, configure MVC and add WebSockets and SignalR.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    public void Configure(IApplicationBuilder app)
    {
        app.UseStaticFiles();

        app.UseMvc(routes =&amp;gt; 
        {
            routes.MapRoute(
                    name: &amp;quot;default&amp;quot;,
                    template: &amp;quot;api/{controller}/{action}/{id?}&amp;quot;
            );
        });

        app.UseWebSockets();
        app.UseSignalR();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calling-client-methods-outside-hubs&#34;&gt;Calling client methods outside hubs&lt;/h2&gt;

&lt;p&gt;In the previous versions of SignalR, in order to call client methods and manage groups outside a hub you would need to make use of the &lt;code&gt;GlobalHost&lt;/code&gt;, which is no longer available in the new version.&lt;/p&gt;

&lt;p&gt;So we will use an instance of &lt;code&gt;ConnectionManager&lt;/code&gt;, specifically the &lt;code&gt;GetHubContext&amp;lt;&amp;gt;()&lt;/code&gt; method. We need to register this service so we can use it in the controller:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_connectionManager.GetHubContext&amp;lt;PostsHub&amp;gt;().someClientMethod(parameters)&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-configureservices-method&#34;&gt;The &lt;code&gt;ConfigureServices&lt;/code&gt; method&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;    public void ConfigureServices(IServiceCollection services)
    {
        var settings = new JsonSerializerSettings();
        settings.ContractResolver = new SignalRContractResolver();

        var serializer = JsonSerializer.Create(settings);

        services.Add(new ServiceDescriptor(typeof(JsonSerializer), 
                     provider =&amp;gt; serializer, 
                     ServiceLifetime.Transient));

        services.AddSingleton&amp;lt;IPostRepository, PostRepository&amp;gt;();

        services.AddSignalR(options =&amp;gt; 
        {
            options.Hubs.EnableDetailedErrors = true;
        });
        
        services.AddMvc();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We added the new contract resolver, we added the &lt;code&gt;PostRepository&lt;/code&gt; and we added SignalR. Now we need to use it in the controller:&lt;/p&gt;

&lt;h2 id=&#34;the-updated-controller&#34;&gt;The updated controller&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;using System.Collections.Generic;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.SignalR.Infrastructure;

public class PostsController : Controller
{
    private IPostRepository _postRepository { get; set; }
    private IConnectionManager _connectionManager {get; set; }

    public PostsController(IPostRepository postRepository, IConnectionManager connectionManager)
    {
        _postRepository = postRepository;
        _connectionManager = connectionManager;
    }

    [HttpGet]
    public List&amp;lt;Post&amp;gt; GetPosts()
    {
        return _postRepository.GetAll();
    }

    [HttpGet]
    public Post GetPost(int id)
    {
        return _postRepository.GetPost(id);
    }

    [HttpPost]
    public void AddPost(Post post)
    {
        _postRepository.AddPost(post);
        _connectionManager.GetHubContext&amp;lt;PostsHub&amp;gt;().Clients.All.publishPost(post);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Every time someone adds a new post, all connected users will be notified and will call the &lt;code&gt;publishPost&lt;/code&gt; method.&lt;/p&gt;

&lt;h2 id=&#34;the-client&#34;&gt;The client&lt;/h2&gt;

&lt;p&gt;The client will be exactly the same as in SignalR 2.2.x:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
    &amp;lt;meta charset=&amp;quot;utf-8&amp;quot; /&amp;gt;

    &amp;lt;script src=&amp;quot;http://ajax.aspnetcdn.com/ajax/jQuery/jquery-2.2.0.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;http://ajax.aspnetcdn.com/ajax/signalr/jquery.signalr-2.2.0.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;/signalr/hubs&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    
    &amp;lt;input id=&amp;quot;userNameInput&amp;quot; type=&amp;quot;text&amp;quot; placeholder=&amp;quot;Enter your user name...&amp;quot; /&amp;gt;
    &amp;lt;input id=&amp;quot;textInput&amp;quot; type=&amp;quot;text&amp;quot; placeholder=&amp;quot;Enter your status...&amp;quot; /&amp;gt;

    &amp;lt;button id=&amp;quot;publishPostButton&amp;quot;&amp;gt;Publish post!&amp;lt;/button&amp;gt;

    &amp;lt;ul id=&amp;quot;postsList&amp;quot;&amp;gt;&amp;lt;/ul&amp;gt;
    
    &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
        $.ajax({
            url: &#39;/api/Posts/GetPosts&#39;,
            method: &#39;GET&#39;,
            dataType: &#39;JSON&#39;,
            success: addPostsList
        });

        function addPostsList(posts) {
            $.each(posts, function (index) {
                var post = posts[index];
                addPost(post);
            });
        }

        function addPost(post) {
            $(&amp;quot;#postsList&amp;quot;).append(
                    &#39;&amp;lt;li&amp;gt;&amp;lt;b&amp;gt;&#39; + post.userName + &#39;&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;&#39; + post.text + &#39;&amp;lt;/li&amp;gt;&amp;lt;br&amp;gt;&#39;
                 );
        }

        var hub = $.connection.postsHub;

        hub.client.publishPost = addPost;

        $(&amp;quot;#publishPostButton&amp;quot;).click(function () {

            var post = {
                userName: $(&amp;quot;#userNameInput&amp;quot;).val() || &amp;quot;Guest&amp;quot;,
                text: $(&amp;quot;#textInput&amp;quot;).val()
            };
            $.ajax({
                url: &#39;/api/Posts/AddPost&#39;,
                method: &#39;POST&#39;,
                data: post
            });
        });
        
        $.connection.hub.logging = true;
        $.connection.hub.start();
    &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if you open two browser tabs and start adding messages, you can see all pages updating in real time.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article we saw how to use MVC Core and SignalR for providing real time data to users.&lt;/p&gt;

&lt;p&gt;Since SignalR is still in alpha, at this moment there are some issues to be addressed and many breaking changes to come and I will try to keep this example up-to-date.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>